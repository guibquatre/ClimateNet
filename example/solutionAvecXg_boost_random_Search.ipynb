{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b0OR-5W8dS4",
        "outputId": "bd7d60d3-93e2-4cf9-86d9-1e190d996bec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import os\n",
        "import logging\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "class DataLoader:\n",
        "    @staticmethod\n",
        "    def safe_load_csv_dataset(file_path: str, is_train=True):\n",
        "        try:\n",
        "            with open(file_path, 'r') as file:\n",
        "                reader = csv.reader(file)\n",
        "                next(reader)  # Skip header row\n",
        "                data = np.array([row for row in reader], dtype=float)\n",
        "\n",
        "            if is_train:\n",
        "                # If it's the training data, assume labels are in the last column\n",
        "                labels = data[:, -1]\n",
        "                features = data[:, :-1]\n",
        "            else:\n",
        "                # If it's the testing data, there are no labels\n",
        "                labels = None\n",
        "                features = data\n",
        "\n",
        "            return features, labels\n",
        "\n",
        "        except FileNotFoundError as e:\n",
        "            logging.error(f\"File not found: {str(e)}\")\n",
        "            return None, None\n",
        "        except ValueError as e:\n",
        "            logging.error(f\"Value error: {str(e)}\")\n",
        "            return None, None\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/climateDoc/classification-of-extreme-weather-events-udem\"\n",
        "file_names = {'train': 'train.csv', 'test': 'test.csv'}\n",
        "\n",
        "loader = DataLoader()\n",
        "train_data, train_labels = loader.safe_load_csv_dataset(os.path.join(data_path, file_names['train']), True)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [50, 100, 200, 500, 1000],\n",
        "    'learning_rate': [0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3],\n",
        "    'max_depth': [3, 4, 5, 6, 7, 8, 10, 12],\n",
        "    'colsample_bytree': [0.3, 0.5, 0.7, 0.9, 1],\n",
        "    'subsample': [0.3, 0.5, 0.7, 0.9, 1],\n",
        "    'gamma': [0, 0.1, 0.2, 0.3, 0.5, 1, 2],\n",
        "    'min_child_weight': [1, 2, 3, 4, 5, 6],\n",
        "    'reg_alpha': [0, 0.1, 0.5, 1, 2],  # L1 regularization term on weights\n",
        "    'reg_lambda': [1, 1.5, 2, 3, 4.5]  # L2 regularization term on weights\n",
        "}\n",
        "\n",
        "xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
        "\n",
        "f1_scorer = make_scorer(f1_score, average='weighted')\n",
        "\n",
        "# Using RandomizedSearchCV with the XGBoost classifier\n",
        "random_search_xgb = RandomizedSearchCV(estimator=xgb_clf, param_distributions=param_grid_xgb,\n",
        "                                      n_iter=200,  # increased iterations\n",
        "                                      scoring=f1_scorer, cv=5, verbose=4, n_jobs=-1, random_state=42)\n",
        "\n",
        "logging.info(\"Starting Randomized Search for XGBoost...\")\n",
        "\n",
        "random_search_xgb.fit(X_train, y_train)\n",
        "\n",
        "logging.info(\"Randomized Search for XGBoost complete.\")\n",
        "\n",
        "joblib.dump(random_search_xgb, '/content/drive/MyDrive/climateDoc/saved_models/random_search_xgb.joblib')\n",
        "\n",
        "logging.info(\"Saved RandomizedSearchCV object for XGBoost to random_search_xgb.joblib.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6663tgVb6MvZ",
        "outputId": "acb1b117-0bf2-401d-f0b6-1d309439edd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib  # saving and loading Python objects efficiently.\n",
        "import os  # providing a way of using operating system dependent functionality.\n",
        "from sklearn.ensemble import RandomForestClassifier  # Import the RandomForest algorithm.\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split  # tools for hyperparameter tuning and splitting\n",
        "from sklearn.metrics import make_scorer, f1_score  # tools for custom scoring function and the F1 score metric.\n",
        "import logging  # Import logging to provide event logging to sys.stderr.\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class DataLoader:\n",
        "    @staticmethod\n",
        "    def safe_load_csv_dataset(file_path: str, is_train=True):\n",
        "        try:\n",
        "            with open(file_path, 'r') as file:\n",
        "                reader = csv.reader(file)\n",
        "                next(reader)  # Skip header row\n",
        "                data = np.array([row for row in reader], dtype=float)\n",
        "\n",
        "            if is_train:\n",
        "                # If it's the training data, assume labels are in the last column\n",
        "                labels = data[:, -1]\n",
        "                features = data[:, :-1]\n",
        "            else:\n",
        "                # If it's the testing data, there are no labels\n",
        "                labels = None\n",
        "                features = data\n",
        "\n",
        "            return features, labels\n",
        "\n",
        "        except FileNotFoundError as e:\n",
        "            logging.error(f\"File not found: {str(e)}\")\n",
        "            return None, None\n",
        "        except ValueError as e:\n",
        "            logging.error(f\"Value error: {str(e)}\")\n",
        "            return None, None\n",
        "\n",
        "\n",
        "# Configure logging to log informational messages.\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Define the path to the data.\n",
        "data_path = \"/content/drive/MyDrive/climateDoc/classification-of-extreme-weather-events-udem\"\n",
        "file_names = {'train': 'train.csv', 'test': 'test.csv'}\n",
        "\n",
        "# Instantiate a DataLoader object.\n",
        "loader = DataLoader()\n",
        "\n",
        "# Use os.path.join to construct a pathname with the path and filename.\n",
        "# Load the training data and labels using the custom DataLoader class.\n",
        "train_data, train_labels = loader.safe_load_csv_dataset(os.path.join(data_path, file_names['train']), True)\n",
        "\n",
        "# Split the training data into training and validation subsets.\n",
        "# 80% of the data is used for training and 20% is used for validation.\n",
        "# random_state is a seed value to ensure reproducibility between runs.\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    train_data,\n",
        "    train_labels,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define an expanded grid of hyperparameters for tuning the RandomForestClassifier.\n",
        "# This grid will be explored during the grid search to find the best performing set of hyperparameters.\n",
        "param_grid_rf = {\n",
        "    # More trees may increase accuracy but also computational cost.\n",
        "    'n_estimators': [10, 50, 100, 200, 300],  # Number of trees in the forest.\n",
        "    # Criterion to split on at each node.\n",
        "    # 'gini' refers to Gini Impurity which is a measure of misclassification,\n",
        "    # indicating how mixed the classes are in two groups created by a potential split.\n",
        "    # A Gini Impurity of 0 indicates perfect separation of classes.\n",
        "    # 'entropy' refers to Information Gain which measures the reduction in entropy (disorder)\n",
        "    # achieved by partitioning the dataset.\n",
        "    # A higher information gain indicates a better split that results in purer subgroups.\n",
        "    # Both 'gini' and 'entropy' are heuristics used to select the best split at each node by\n",
        "    # evaluating the splits on all features and all possible threshold values for those features.\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [None, 10, 20, 30, 40],  # Maximum depth of the trees.\n",
        "    # None means nodes are expanded until they contain less than min_samples_split samples.\n",
        "    'min_samples_split': [2, 5, 10, 15],  # Minimum number of samples required to split an internal node.\n",
        "    'min_samples_leaf': [1, 2, 4, 8],  # Minimum number of samples required to be at a leaf node.\n",
        "    'bootstrap': [True, False],  # Method for sampling data points (with or without replacement).\n",
        "    'class_weight': [None, 'balanced', 'balanced_subsample'],  # Weights associated with classes\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],  # The number of features to consider when looking for the best split.\n",
        "    'max_leaf_nodes': [None, 10, 50, 100],  # Grow trees with a certain maximum number of leaf nodes.\n",
        "    # Splitting node only if this split induces a decrease of the impurity greater than or equal to this value.\n",
        "    'min_impurity_decrease': [0.0, 0.01, 0.05]\n",
        "}\n",
        "\n",
        "# Instantiate a RandomForestClassifier object with a fixed random state for reproducibility.\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Create a custom scoring function using the make_scorer function and the F1 score metric.\n",
        "# The F1 score is a measure model's precision and recall.\n",
        "f1_scorer = make_scorer(f1_score, average='weighted')\n",
        "\n",
        "# Instantiate a GridSearchCV object to perform a grid search of the RandomForestClassifier hyperparameters.\n",
        "# This object will explore the parameter grid using cross-validation to find the best set of hyperparameters.\n",
        "grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf,\n",
        "                              scoring=f1_scorer,  # Use the custom F1 scoring function.\n",
        "                              cv=5,  # Perform 5-fold cross-validation.\n",
        "                              verbose=4,  # Output messages to the console.\n",
        "                              n_jobs=-1)  # Use all available cores on the machine for parallel processing.\n",
        "\n",
        "# Log the start of the grid search process to the console.\n",
        "logging.info(\"Starting Grid Search...\")\n",
        "\n",
        "# Fit the GridSearchCV object to the training data.\n",
        "# train a RandomForestClassifier for each combination of hyperparameters in the grid,\n",
        "# and evaluate them using cross-validation.\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "# Log the completion of the grid search process to the console.\n",
        "logging.info(\"Grid Search complete.\")\n",
        "\n",
        "# Save the fitted GridSearchCV object to disk for later use.\n",
        "# This object contains the best set of hyperparameters found during the grid search,\n",
        "# as well as the fitted RandomForestClassifier with those hyperparameters.\n",
        "joblib.dump(grid_search_rf, '/content/drive/MyDrive/climateDoc/big_grid_search_rf.joblib')\n",
        "\n",
        "# Log the saving of the GridSearchCV object to the console.\n",
        "logging.info(\"Saved GridSearchCV object to grid_search_rf.joblib.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6Ikb84ar6fU",
        "outputId": "a93e45a6-eeb6-4f9b-bafc-d92df1ded9fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 172800 candidates, totalling 864000 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import os\n",
        "import logging\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "class DataLoader:\n",
        "    @staticmethod\n",
        "    def safe_load_csv_dataset(file_path: str, is_train=True):\n",
        "        try:\n",
        "            with open(file_path, 'r') as file:\n",
        "                reader = csv.reader(file)\n",
        "                next(reader)  # Skip header row\n",
        "                data = np.array([row for row in reader], dtype=float)\n",
        "\n",
        "            if is_train:\n",
        "                # If it's the training data, assume labels are in the last column\n",
        "                labels = data[:, -1]\n",
        "                features = data[:, :-1]\n",
        "            else:\n",
        "                # If it's the testing data, there are no labels\n",
        "                labels = None\n",
        "                features = data\n",
        "\n",
        "            return features, labels\n",
        "\n",
        "        except FileNotFoundError as e:\n",
        "            logging.error(f\"File not found: {str(e)}\")\n",
        "            return None, None\n",
        "        except ValueError as e:\n",
        "            logging.error(f\"Value error: {str(e)}\")\n",
        "            return None, None\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/climateDoc/classification-of-extreme-weather-events-udem\"\n",
        "file_names = {'train': 'train.csv', 'test': 'test.csv'}\n",
        "\n",
        "loader = DataLoader()\n",
        "train_data, train_labels = loader.safe_load_csv_dataset(os.path.join(data_path, file_names['train']), True)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define an expanded grid of hyperparameters for tuning the RandomForestClassifier.\n",
        "# This grid will be explored during the grid search to find the best performing set of hyperparameters.\n",
        "param_grid_rf = {\n",
        "    # More trees may increase accuracy but also computational cost.\n",
        "    'n_estimators': [10, 50, 100, 200, 300],  # Number of trees in the forest.\n",
        "    # Criterion to split on at each node.\n",
        "    # 'gini' refers to Gini Impurity which is a measure of misclassification,\n",
        "    # indicating how mixed the classes are in two groups created by a potential split.\n",
        "    # A Gini Impurity of 0 indicates perfect separation of classes.\n",
        "    # 'entropy' refers to Information Gain which measures the reduction in entropy (disorder)\n",
        "    # achieved by partitioning the dataset.\n",
        "    # A higher information gain indicates a better split that results in purer subgroups.\n",
        "    # Both 'gini' and 'entropy' are heuristics used to select the best split at each node by\n",
        "    # evaluating the splits on all features and all possible threshold values for those features.\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [None, 10, 20, 30, 40],  # Maximum depth of the trees.\n",
        "    # None means nodes are expanded until they contain less than min_samples_split samples.\n",
        "    'min_samples_split': [2, 5, 10, 15],  # Minimum number of samples required to split an internal node.\n",
        "    'min_samples_leaf': [1, 2, 4, 8],  # Minimum number of samples required to be at a leaf node.\n",
        "    'bootstrap': [True, False],  # Method for sampling data points (with or without replacement).\n",
        "    'class_weight': [None, 'balanced', 'balanced_subsample'],  # Weights associated with classes\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],  # The number of features to consider when looking for the best split.\n",
        "    'max_leaf_nodes': [None, 10, 50, 100],  # Grow trees with a certain maximum number of leaf nodes.\n",
        "    # Splitting node only if this split induces a decrease of the impurity greater than or equal to this value.\n",
        "    'min_impurity_decrease': [0.0, 0.01, 0.05]\n",
        "}\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "f1_scorer = make_scorer(f1_score, average='weighted')\n",
        "\n",
        "# Using RandomizedSearchCV instead of GridSearchCV\n",
        "random_search_rf = RandomizedSearchCV(estimator=rf, param_distributions=param_grid_rf,\n",
        "                                     n_iter=100,  # You can adjust the number of iterations\n",
        "                                     scoring=f1_scorer, cv=5, verbose=4, n_jobs=-1, random_state=42)\n",
        "\n",
        "logging.info(\"Starting Randomized Search...\")\n",
        "\n",
        "random_search_rf.fit(X_train, y_train)\n",
        "\n",
        "logging.info(\"Randomized Search complete.\")\n",
        "\n",
        "joblib.dump(random_search_rf, '/content/drive/MyDrive/climateDoc/saved_models/random_search_rf.joblib')\n",
        "\n",
        "logging.info(\"Saved RandomizedSearchCV object to random_search_rf.joblib.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OU26EvuNSWGz",
        "outputId": "1b10c021-9160-4818-afa9-8b4c4de98e9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import logging\n",
        "import joblib\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "# Keep your logging setup and DataLoader class\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "\n",
        "class DataLoader:\n",
        "    @staticmethod\n",
        "    def safe_load_csv_dataset(file_path: str, is_train=True):\n",
        "        try:\n",
        "            with open(file_path, 'r') as file:\n",
        "                reader = csv.reader(file)\n",
        "                next(reader)  # Skip header row\n",
        "                data = np.array([row for row in reader], dtype=float)\n",
        "\n",
        "            if is_train:\n",
        "                # If it's the training data, assume labels are in the last column\n",
        "                labels = data[:, -1]\n",
        "                features = data[:, :-1]\n",
        "            else:\n",
        "                # If it's the testing data, there are no labels\n",
        "                labels = None\n",
        "                features = data\n",
        "\n",
        "            return features, labels\n",
        "\n",
        "        except FileNotFoundError as e:\n",
        "            logging.error(f\"File not found: {str(e)}\")\n",
        "            return None, None\n",
        "        except ValueError as e:\n",
        "            logging.error(f\"Value error: {str(e)}\")\n",
        "            return None, None\n",
        "\n",
        "\n",
        "class SubmissionSaver:\n",
        "    @staticmethod\n",
        "    def save_submission(predictions, file_path='/content/drive/MyDrive/climateDoc/submission.csv'):\n",
        "        with open(file_path, mode='w', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow(['SNo', 'Label'])\n",
        "            for i, label in enumerate(predictions, 1):\n",
        "                writer.writerow([i, label])\n",
        "        logging.info(f'Submission saved to {file_path}')\n",
        "\n",
        "\n",
        "class ModelSaver:\n",
        "    @staticmethod\n",
        "    def save_model(_model):\n",
        "        try:\n",
        "            os.makedirs(\"saved_models\", exist_ok=True)\n",
        "            class_name = _model.__class__.__name__\n",
        "            timestamp = datetime.now().strftime(\"%d_%H_%M_%S\")\n",
        "            filename = f\"saved_models/{class_name}_{timestamp}.joblib\"\n",
        "            joblib.dump(_model, filename)\n",
        "            logging.info(f\"Model saved to {filename}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error saving model: {str(e)}\")\n",
        "\n",
        "\n",
        "class Preprocessor:\n",
        "    def __init__(self):\n",
        "        self.mean = None\n",
        "        self.std = None\n",
        "        self.min = None\n",
        "        self.max = None\n",
        "\n",
        "    # Compute the values along each column, independently\n",
        "    def fit(self, X):\n",
        "        # Compute and store the mean of each feature/column in the dataset\n",
        "        self.mean = np.mean(X, axis=0)\n",
        "        # Compute and store the standard deviation of each feature/column in the dataset\n",
        "        self.std = np.std(X, axis=0)\n",
        "        # Compute and store the minimum value of each feature/column in the dataset\n",
        "        self.min = np.min(X, axis=0)\n",
        "        # Compute and store the maximum value of each feature/column in the dataset\n",
        "        self.max = np.max(X, axis=0)\n",
        "\n",
        "    # Method to apply either standardization or normalization to the dataset\n",
        "    def transform(self, X, method='standardize'):\n",
        "        # Check the method argument to determine the transformation to apply\n",
        "        if method == 'standardize':\n",
        "            # Call the standardize method (not defined in provided code) to standardize the dataset\n",
        "            return self.standardize(X)\n",
        "        elif method == 'normalize':\n",
        "            # Call the normalize method to normalize the dataset\n",
        "            return self.normalize(X)\n",
        "        else:\n",
        "            # If an unknown method is provided, raise a ValueError with a descriptive message\n",
        "            raise ValueError(f\"Unknown method: {method}\")\n",
        "\n",
        "    def standardize(self, X):\n",
        "        # Standardization formula: (X - mean) / std\n",
        "        # Ensure to add a small value to the denominator to avoid division by zero\n",
        "        return (X - self.mean) / (self.std + 1e-8)\n",
        "\n",
        "    def normalize(self, X):\n",
        "        # Normalization formula: (X - min) / (max - min)\n",
        "        # Ensure to add a small value to the denominator to avoid division by zero\n",
        "        return (X - self.min) / (self.max - self.min + 1e-8)\n",
        "\n",
        "\n",
        "class SimpleDummyClassifier:\n",
        "    def __init__(self):\n",
        "        self.unique_labels = None\n",
        "\n",
        "    def fit(self, _: np.array, y: np.array) -> None:\n",
        "        self.unique_labels = np.unique(y)\n",
        "\n",
        "    def predict(self, _: np.array) -> np.array:\n",
        "        num_samples = _.shape[0]\n",
        "        return np.random.choice(self.unique_labels, size=num_samples)\n",
        "\n",
        "\n",
        "class SoftLogisticRegression:\n",
        "    # initialize the hyperparameters and sets up the initial values of weights and bias.\n",
        "    def __init__(self, learning_rate=0.01, num_iterations=1000, regularization_strength=0.1):\n",
        "        self.learning_rate = learning_rate  # step size used during optimization to find the minimum of loss function.\n",
        "        self.num_iterations = num_iterations  # number of steps the optimizer will take to minimize loss function.\n",
        "        self.regularization_strength = regularization_strength  # Controls regularization strength, prevent overfitting.\n",
        "        self.weights = None  # Placeholder for the weights vector that will be learned from the data.\n",
        "        self.bias = None  # Placeholder for the bias term that will be learned from the data.\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(z):\n",
        "        # For numerical stability, subtract the maximum value of z for each sample.\n",
        "        z -= np.max(z, axis=1, keepdims=True)\n",
        "        # Compute the exponential of z to get unnormalized probabilities.\n",
        "        exp_z = np.exp(z)\n",
        "        # Sum the unnormalized probabilities for each sample to normalize them.\n",
        "        sum_exp_z = np.sum(exp_z, axis=1, keepdims=True)\n",
        "        # Divide each unnormalized probability by the sum to get the normalized probabilities.\n",
        "        return exp_z / sum_exp_z\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        y = y.astype(int)  # Convert the labels to integer type in case they are not for indexing\n",
        "        num_samples, num_features = X.shape\n",
        "        num_classes = len(np.unique(y))  # Get the number of unique labels, which equals the number of classes.\n",
        "        self.weights = np.zeros((num_features, num_classes))  # Initialize the weights matrix with zeros.\n",
        "        self.bias = np.zeros(num_classes)  # Initialize the bias vector with zeros.\n",
        "        y_one_hot = np.eye(num_classes)[y]\n",
        "        for _ in range(self.num_iterations):\n",
        "            linear_model = np.dot(X, self.weights) + self.bias\n",
        "            probabilities = self.softmax(linear_model)\n",
        "            error = probabilities - y_one_hot\n",
        "            gradient_weights = (1 / num_samples) * np.dot(X.T, error) + self.regularization_strength * self.weights\n",
        "            gradient_bias = (1 / num_samples) * np.sum(error, axis=0)\n",
        "            self.weights -= self.learning_rate * gradient_weights\n",
        "            self.bias -= self.learning_rate * gradient_bias\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        probabilities = self.softmax(linear_model)\n",
        "        return np.argmax(probabilities, axis=1)\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, label):\n",
        "    tp = np.sum((y_true == label) & (y_pred == label))\n",
        "    fp = np.sum((y_true != label) & (y_pred == label))\n",
        "    fn = np.sum((y_true == label) & (y_pred != label))\n",
        "    precision = tp / (tp + fp) if tp + fp > 0 else 0.0\n",
        "    recall = tp / (tp + fn) if tp + fn > 0 else 0.0\n",
        "    # Step 6: Calculate F1 Score\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n",
        "    return precision, recall, f1\n",
        "\n",
        "\n",
        "def classification_report_custom(y_true, y_pred):\n",
        "\n",
        "    labels = np.unique(y_true)\n",
        "\n",
        "    # Step 2: Iterate Over Each Unique Label\n",
        "    for label in labels:\n",
        "        precision, recall, f1 = calculate_metrics(y_true, y_pred, label)\n",
        "        # Step 3: Display Metrics for Each Label\n",
        "        print(f'Label: {label}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1:.2f}')\n",
        "\n",
        "\n",
        "class ClimateAnalysisPipeline:\n",
        "    def __init__(self):\n",
        "        logging.info(\"Initializing ClimateAnalysisPipeline\")\n",
        "        self.training_set = {'data': None, 'labels': None}\n",
        "        self.inference_set = {'data': None, 'labels': None}\n",
        "\n",
        "    def load_datasets(self, _path, _file_names):\n",
        "        loader = DataLoader()\n",
        "        self.training_set['data'], self.training_set['labels'] = \\\n",
        "            loader.safe_load_csv_dataset(os.path.join(_path, _file_names['train']), True)\n",
        "        self.inference_set['data'], _ = \\\n",
        "            loader.safe_load_csv_dataset(os.path.join(_path, _file_names['test']), False)\n",
        "\n",
        "    def train_and_evaluate(self):\n",
        "        logging.info(\"Starting model training and evaluation\")\n",
        "        if self.training_set['data'] is not None:\n",
        "            X_train, X_val, y_train, y_val = train_test_split(\n",
        "                self.training_set['data'],\n",
        "                self.training_set['labels'],\n",
        "                test_size=0.2, random_state=42\n",
        "            )\n",
        "\n",
        "            # Adding XGBoost to the model list\n",
        "            xg_reg = xgb.XGBClassifier(objective='binary:logistic', colsample_bytree=0.3, learning_rate=0.1,\n",
        "                                       max_depth=5, alpha=10, n_estimators=10)\n",
        "            # Run gridsearch.py before this line\n",
        "            grid_search_rf = joblib.load('/content/drive/MyDrive/climateDoc/saved_models/random_search_rf.joblib')\n",
        "            # Get the best estimator from the GridSearch\n",
        "            best_rf = grid_search_rf.best_estimator_\n",
        "\n",
        "              # Run gridsearch.py before this line\n",
        "            grid_search_xgb = joblib.load('/content/drive/MyDrive/climateDoc/saved_models/random_search_xgb.joblib')\n",
        "            # Get the best estimator from the GridSearch\n",
        "            best_xgb = grid_search_xgb.best_estimator_\n",
        "\n",
        "\n",
        "            baseline_models = [\n",
        "                ('SimpleDummy', SimpleDummyClassifier()),\n",
        "                ('SoftLogisticRegression', SoftLogisticRegression()),\n",
        "                ('Dummy', DummyClassifier(strategy=\"uniform\")),\n",
        "                ('SGD', SGDClassifier(class_weight='balanced')),\n",
        "                ('SVC', SVC(class_weight='balanced')),\n",
        "                ('RandomForest_best_rf', best_rf),\n",
        "                ('RandomForest', RandomForestClassifier(class_weight='balanced')),\n",
        "                ('LogisticRegression', LogisticRegression(max_iter=1000, class_weight='balanced')),\n",
        "                ('XGBoost', xg_reg),\n",
        "                ('XGBoost_best_xgb', xg_reg)\n",
        "            ]\n",
        "\n",
        "            best_f1 = 0.0\n",
        "            best_model_name = \"\"\n",
        "            _best_model = None\n",
        "\n",
        "            for name, model in baseline_models:\n",
        "                model.fit(X_train, y_train)\n",
        "                _predictions = model.predict(X_val)\n",
        "                print(f'Performance of {name}:')\n",
        "                classification_report_custom(y_val, _predictions)\n",
        "                with warnings.catch_warnings():\n",
        "                    warnings.simplefilter(\"ignore\", category=UndefinedMetricWarning)\n",
        "                    report = classification_report(y_val, _predictions, output_dict=True, zero_division=0)\n",
        "                f1 = report['weighted avg']['f1-score']\n",
        "                logging.info(\"Model {} F1-score: {:.4f}\".format(name, f1))\n",
        "\n",
        "                if f1 > best_f1:\n",
        "                    best_f1 = f1\n",
        "                    best_model_name = name\n",
        "                    _best_model = model\n",
        "\n",
        "            logging.info(\"Best model is {} with F1-score of {:.4f}\".format(best_model_name, best_f1))\n",
        "            ModelSaver.save_model(_best_model)\n",
        "            return _best_model\n",
        "        else:\n",
        "            logging.error(\"Training data is missing. Cannot proceed.\")\n",
        "            return None\n",
        "\n",
        "\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/climateDoc/classification-of-extreme-weather-events-udem\"\n",
        "file_names = {'train': 'train.csv', 'test': 'test.csv'}\n",
        "\n",
        "loader = DataLoader()\n",
        "train_data, train_labels = loader.safe_load_csv_dataset(os.path.join(data_path, file_names['train']), True)\n",
        "test_data, _ = loader.safe_load_csv_dataset(os.path.join(data_path, file_names['test']), False)\n",
        "\n",
        "if train_data is None or test_data is None:\n",
        "    logging.error(\"Data loading failed. Cannot proceed.\")\n",
        "    exit(1)\n",
        "\n",
        "preprocessor = Preprocessor()\n",
        "preprocessor.fit(train_data)  # Compute statistics based on the training data\n",
        "# Standardize the training data\n",
        "train_data = preprocessor.transform(train_data, method='standardize')\n",
        "# Standardize the test data using the same statistics\n",
        "test_data = preprocessor.transform(test_data, method='standardize')\n",
        "\n",
        "# Create an instance of ClimateAnalysisPipeline\n",
        "pipeline = ClimateAnalysisPipeline()\n",
        "\n",
        "# Set the training and inference datasets\n",
        "pipeline.training_set['data'], pipeline.training_set['labels'] = train_data, train_labels\n",
        "pipeline.inference_set['data'] = test_data\n",
        "\n",
        "# Call train_and_evaluate\n",
        "best_model = pipeline.train_and_evaluate()\n",
        "\n",
        "if best_model:\n",
        "    test_predictions = best_model.predict(test_data)\n",
        "    SubmissionSaver.save_submission(test_predictions)\n",
        "else:\n",
        "    logging.error(\"No best model, sorry\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSEChmhxgxcq",
        "outputId": "1925f3c0-9a8f-4429-f04f-a7cca937e77f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance of SimpleDummy:\n",
            "Label: 0.0, Precision: 0.79, Recall: 0.34, F1: 0.47\n",
            "Label: 1.0, Precision: 0.04, Recall: 0.35, F1: 0.08\n",
            "Label: 2.0, Precision: 0.17, Recall: 0.33, F1: 0.22\n",
            "Performance of SoftLogisticRegression:\n",
            "Label: 0.0, Precision: 0.81, Recall: 0.98, F1: 0.89\n",
            "Label: 1.0, Precision: 0.72, Recall: 0.04, F1: 0.07\n",
            "Label: 2.0, Precision: 0.63, Recall: 0.13, F1: 0.21\n",
            "Performance of Dummy:\n",
            "Label: 0.0, Precision: 0.80, Recall: 0.33, F1: 0.47\n",
            "Label: 1.0, Precision: 0.04, Recall: 0.33, F1: 0.07\n",
            "Label: 2.0, Precision: 0.17, Recall: 0.34, F1: 0.23\n",
            "Performance of SGD:\n",
            "Label: 0.0, Precision: 0.88, Recall: 0.84, F1: 0.86\n",
            "Label: 1.0, Precision: 0.28, Recall: 0.60, F1: 0.38\n",
            "Label: 2.0, Precision: 0.58, Recall: 0.55, F1: 0.56\n",
            "Performance of SVC:\n",
            "Label: 0.0, Precision: 0.98, Recall: 0.76, F1: 0.86\n",
            "Label: 1.0, Precision: 0.41, Recall: 0.96, F1: 0.57\n",
            "Label: 2.0, Precision: 0.54, Recall: 0.91, F1: 0.67\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance of RandomForest_best_rf:\n",
            "Label: 0.0, Precision: 0.98, Recall: 0.86, F1: 0.92\n",
            "Label: 1.0, Precision: 0.59, Recall: 0.90, F1: 0.72\n",
            "Label: 2.0, Precision: 0.64, Recall: 0.90, F1: 0.75\n",
            "Performance of RandomForest:\n",
            "Label: 0.0, Precision: 0.91, Recall: 0.91, F1: 0.91\n",
            "Label: 1.0, Precision: 0.66, Recall: 0.59, F1: 0.62\n",
            "Label: 2.0, Precision: 0.64, Recall: 0.64, F1: 0.64\n",
            "Performance of LogisticRegression:\n",
            "Label: 0.0, Precision: 0.95, Recall: 0.58, F1: 0.72\n",
            "Label: 1.0, Precision: 0.23, Recall: 0.91, F1: 0.36\n",
            "Label: 2.0, Precision: 0.41, Recall: 0.85, F1: 0.55\n",
            "Performance of XGBoost:\n",
            "Label: 0.0, Precision: 0.85, Recall: 0.97, F1: 0.91\n",
            "Label: 1.0, Precision: 0.83, Recall: 0.35, F1: 0.49\n",
            "Label: 2.0, Precision: 0.77, Recall: 0.35, F1: 0.48\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}